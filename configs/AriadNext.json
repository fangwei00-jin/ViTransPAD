{
    "data_loader": {
        "w": 224,
        "h": 224,
        "sample_length": 8,
        "Transformer_layers": 4,
        "Transformer_heads": [2,3],
        "##**backbone: EfficientNet patches [28,14,7] *** Vit(from vit_pytorch) patches [32,16,8,4,2] *** Vit(from timme) patches [16,8,4,2] ***":"",
        "patchsize":[112,56,28,14,7,4,2],
        "#patchsize":[32,16,8,4,2],
        "#patchsize":[16,8,4,2],
        "#model": "sttn_vit",
        "#model": "sttn_CNN",
        "model": "sttn_transformer",
        "#model": "sttn_transformer_attent_FC",
        "#model": "sttn_transformer_attent_FC_NoEncoder",
        "#model": "sttn_transformer_attent_Vit",
        "#model": "sttn_transformer_Efficientnet_FrameAtten",
        "backbone": "EfficientNet",
        "#backbone": "Vit_pretrained",
        "#backbone": "Vit",
        "featurelayer": "model_backbone._conv_head",
        "#featurelayer": "model_backbone._blocks[15]._bn2",
        "#Transformer output's shape should be same as input emebedding (dimension/channel, size) enabling the following concatenation x+att(x)": "",
        "##EfficientNet channel 80 *** Vit from vit_pytorch channel 65 *** Vit from timm channel 3": "",
        "channel": 80,
        "#channel": 16,
        "#channel": 65,
        "#channel": 3,
        "#mode": "dataset_image",
        "mode": "dataset_video",
        "##class is 2 for standard binary Bonafide|Attack classification or X for fine-grainde attack type classification": "",
        "class": 2
    },
    "train": {
        "seed": 42,
        "#data_root": "/data/zming/datasets/Anti-spoof/OuluNPU/Train_files",
        "data_root": "/data/zming/datasets/Anti-spoof/SELFCOLLECT_Data/AriadNext/Dataset_PAD_L3i_v1_14_04_2021/train",
        "save_dir": "/data/zming/models/Depth_antispoof/tmp_test/",
        "#modality can include one or two modality for training the model": "",
        "#modality": ["train_protocol2.json"],
        "modality": ["data.json"],
        "train_module": "core.trainer_transformer",
        "#pretrained_model": "",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp38/sttn_vit_OuluNPU/gen_00160.pth",
        "pretrained_model": "/data/zming/models/Depth_antispoof/Transformer_Depth/ICCASP2022/Ablation/1_CNN_vs_Transformer/protocol2/Efficientnet/gen_00025.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp1/sttn_transformer_Efficientnet_OuluNPU/gen_00025.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp7/sttn_transformer_Efficientnet_OuluNPU/gen_00035.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp8/sttn_transformer_Efficientnet_OuluNPU/gen_00015.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp9/sttn_transformer_Efficientnet_OuluNPU/gen_00080.pth",
        "#pretrained_model": "/data/guests/l3i/ming_01/models/Depth_antispoof/pretrained/tmp15/sttn_transformer_OuluNPU/gen_00045.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp36/sttn_transformer_OuluNPU/gen_00015.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp2/sttn_transformer_Efficientnet_FrameAtten_OuluNPU/gen_00025.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp7/sttn_transformer_Efficientnet_OuluNPU/gen_00035.pth",
        "#pretrained_model": "/data/zming/mobidem/model_yaga/tmp148/sttn_transformer_attent_FC_OuluNPU/gen_00085.pth",
        "face_scale": [1.2, 1.5],
         "losses": {
            "hole_weight": 1,
            "valid_weight": 1,
            "adversarial_weight": 0.01,
            "GAN_LOSS": "hinge"
        },
        "trainer": {
            "type": "Adam",
            "beta1": 0,
            "beta2": 0.99,
            "lr": 5e-5,
            "d2glr": 1,
            "#batch_size": 64,
            "batch_size": 1,
            "#num_workers": 16,
            "num_workers": 0,
            "verbosity": 2,
            "log_step": 100,
            "save_freq": 268,
            "#valid_freq": 1e4,
            "valid_freq": 10,
            "eval_epoch_num": 5,
            "iterations": 50e4,
            "niter": 15e4,
            "niter_steady": 30e4
        }
    },

    "test": {
        "#test_data": "/data/zming/datasets/Anti-spoof/OuluNPU/Test_files/test_protocol2.json",
        "test_data": "/data/zming/datasets/Anti-spoof/SELFCOLLECT_Data/AriadNext/Dataset_PAD_L3i_v1_14_04_2021/train/data.json",
        "#val_data": "/data/zming/datasets/Anti-spoof/OuluNPU/Dev_files/dev_protocol2.json",
        "val_data": "/data/zming/datasets/Anti-spoof/SELFCOLLECT_Data/AriadNext/Dataset_PAD_L3i_v1_14_04_2021/train/data.json",
        "ckpt": "",
        "save_dir": "",
        "face_scale": 1.3,
        "test_module": "core.eval_transformer"
    }
}
